import os
import tweepy
import feedparser
import re
import openai
import json
import time
import random
from datetime import datetime, timedelta

# =========================================================
#              ENV + CONSTANTS + BOOT GUARDS
# =========================================================

TWITTER_API_KEY = os.getenv("TWITTER_API_KEY")
TWITTER_SECRET = os.getenv("TWITTER_SECRET")
TWITTER_ACCESS_TOKEN = os.getenv("TWITTER_ACCESS_TOKEN")
TWITTER_ACCESS_SECRET = os.getenv("TWITTER_ACCESS_SECRET")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
XAI_API_KEY = os.getenv("XAI_API_KEY")

OPENAI_MODEL = "gpt-5"
XAI_MODEL = "grok-4-fast-reasoning"

# =========================================================
#                        TWITTER
# =========================================================

twitter_client = tweepy.Client(
    consumer_key=TWITTER_API_KEY,
    consumer_secret=TWITTER_SECRET,
    access_token=TWITTER_ACCESS_TOKEN,
    access_token_secret=TWITTER_ACCESS_SECRET
)

# Accounts FloodLink may occasionally reply to (FILL IN REAL IDs)
TARGET_ACCOUNTS = {
    "sama": "1605",        # Replace with actual user IDs
    "elonmusk": "44196397",    # Replace with actual user IDs
    "stats_feed": "1335132884278108161",   # Replace with actual user IDs
    "balajis": "2178012643"
}

# =========================================================
#                         RSS
# =========================================================

# FloodLink-focused Google News RSS feeds
RSS_FEEDS = [
    # Global flood & flash-flood alerts
    "https://news.google.com/rss/search?q=flood+warning+OR+flash+flood&hl=en&gl=US&ceid=US:en",
    "https://news.google.com/rss/search?q=heavy+rain+flooding+OR+river+overflows&hl=en&gl=US&ceid=US:en",
    "https://news.google.com/rss/search?q=dam+break+flood+OR+levee+breach&hl=en&gl=US&ceid=US:en",

    # Tropical cyclones with flooding
    "https://news.google.com/rss/search?q=hurricane+flooding+OR+storm+surge+flooding&hl=en&gl=US&ceid=US:en",
    "https://news.google.com/rss/search?q=typhoon+flooding+OR+cyclone+flooding&hl=en&gl=US&ceid=US:en",

    # Seasonal / monsoon + landslides from heavy rain
    "https://news.google.com/rss/search?q=monsoon+floods+OR+monsoon+flooding&hl=en&gl=IN&ceid=IN:en",
    "https://news.google.com/rss/search?q=landslide+heavy+rain+OR+rainfall+triggered+landslide&hl=en&gl=US&ceid=US:en",

    # (Optional later) Global disaster feeds like GDACS / FloodList
    # "https://www.gdacs.org/xml/rss.xml",
]

# =========================================================
#                     STORAGE + LIMITS
# =========================================================

LOG_FILE = "floodlink_news.json"
REPLY_LOG_FILE = "floodlink_replies.json"

RETENTION_DAYS = 10
TWEET_THRESHOLD = 9  # 0‚Äì10 relevance; post only high-impact events

# Tweet type probabilities (FloodLink: mostly news + a few replies)
RANDOM_NEWS = 0.3
RANDOM_REPLY = 0.15
RANDOM_NONE = 0.65

NEWS_TWEETS_LIMIT = 4   # daily flood news tweets
REPLY_TWEETS_LIMIT = 1  # daily replies

# =========================================================
#                        HELPERS
# =========================================================

STOPWORDS = set([
    "the", "and", "is", "in", "on", "at", "to", "of", "for", "with", "a", "an",
    "this", "that", "from", "by", "as", "it", "its", "was", "were", "are", "be",
    "new", "latest", "after", "before", "during", "amid"
])

def extract_key_terms(text):
    if not text:
        return set()
    text = str(text).lower()
    words = re.findall(r"\b\w+\b", text)
    numbers = re.findall(r"\d+", text)
    keywords = [w for w in words if w not in STOPWORDS] + numbers
    return set(keywords)

def is_similar_news(new_title, new_summary, processed_articles, threshold=0.6, limit=30):
    new_keywords = extract_key_terms(new_title) | extract_key_terms(new_summary)

    # keep only valid, high-score recent ones
    recent_articles = [
        a for a in processed_articles
        if isinstance(a.get("score", 0), (int, float))
        and a.get("score", 0) >= TWEET_THRESHOLD
    ][-limit:]

    for article in recent_articles:
        old_keywords = (
            extract_key_terms(article.get("tweet", "")) |
            extract_key_terms(article.get("title", "")) |
            extract_key_terms(article.get("summary", ""))
        )
        if old_keywords:
            similarity = len(new_keywords & old_keywords) / len(new_keywords | old_keywords)
            if similarity >= threshold:
                print(f"‚ö†Ô∏è Skipping similar news: {new_title} (Similarity: {similarity:.2f})")
                return True
    return False

def load_processed_articles():
    if os.path.exists(LOG_FILE):
        try:
            with open(LOG_FILE, "r") as f:
                data = json.load(f)
            valid = [a for a in data if isinstance(a, dict) and "date" in a]
            print(f"Loaded {len(valid)} processed flood articles.")
            return valid
        except json.JSONDecodeError:
            print("‚ö†Ô∏è Corrupted floodlink_news.json, resetting.")
            return []
    return []

def cleanup_old_articles(processed_articles):
    cutoff = datetime.utcnow() - timedelta(days=RETENTION_DAYS)
    return [
        a for a in processed_articles
        if datetime.strptime(a["date"], "%Y-%m-%d") >= cutoff
    ]

def save_processed_articles(processed):
    print("üíæ Writing to floodlink_news.json...")
    try:
        with open(LOG_FILE, "w") as f:
            json.dump(processed, f, indent=4)
        print("‚úÖ Successfully wrote to floodlink_news.json!")
    except Exception as e:
        print(f"‚ùå Error writing to JSON: {e}")
        return

    if os.getenv("GITHUB_ACTIONS"):
        print("üîÑ Committing changes to GitHub...")
        os.system("git config --global user.email 'github-actions@github.com'")
        os.system("git config --global user.name 'GitHub Actions'")
        os.system("git add floodlink_news.json")
        commit_result = os.system("git commit -m 'Update floodlink_news.json [Automated]'")
        if commit_result != 0:
            print("‚ö†Ô∏è No changes to commit. Skipping push.")
            return
        push_result = os.system("git push origin main")
        if push_result != 0:
            print("‚ùå Push failed, check GitHub Actions permissions.")
        else:
            print("‚úÖ Changes committed to GitHub.")

def select_tweet_type():
    return random.choices(
        ["news", "reply", "none"],
        [RANDOM_NEWS, RANDOM_REPLY, RANDOM_NONE]
    )[0]

def count_news_tweets_today(processed_articles):
    today = datetime.utcnow().strftime("%Y-%m-%d")
    return sum(1 for a in processed_articles if a.get("date") == today and a.get("type") == "news")

# =========================================================
#                   NEWS FETCH + SCORING
# =========================================================

def get_latest_news():
    """
    Fetch recent flood-related stories from RSS feeds.
    By default we accept items from the last 6 hours (tune as needed).
    """
    news_list = []
    now = datetime.utcnow()

    for feed_url in RSS_FEEDS:
        try:
            print(f"üîÑ Fetching news from: {feed_url}")
            feed = feedparser.parse(feed_url)
            if not feed.entries:
                print(f"‚ö†Ô∏è No entries for {feed_url}")
                continue

            for entry in feed.entries:
                title = entry.title
                link = entry.link
                published_time = datetime(*entry.published_parsed[:6]) if "published_parsed" in entry else now
                source = getattr(entry, "source", None).title if hasattr(entry, "source") else "Unknown source"
                summary = getattr(entry, "summary", "") or ""

                if now - published_time < timedelta(hours=6):
                    news_list.append((title, link, source, summary))
        except Exception as e:
            print(f"‚ùå Error fetching feed {feed_url}: {e}")
            continue

    return news_list

# =========================================================
#               AI: SCORING + SUMMARIZATION
# =========================================================

def get_news_relevance_score(title, summary):
    """
    Score how relevant this article is to FloodLink (0‚Äì10).
    High scores = strong, clear flood / flash-flood signal and impact.
    """
    client = openai.OpenAI(
        api_key=XAI_API_KEY,
        base_url="https://api.x.ai/v1"
    )

    prompt = f"""
You are ranking news articles for FloodLink, a global flood-risk early warning system on X.

Assign a relevance score from 0 to 10 for this article, focusing ONLY on:
- river floods
- flash floods
- coastal flooding / storm surge
- flooding from tropical cyclones, typhoons, monsoon rains
- rainfall-triggered landslides
- serious flood preparedness, evacuations, warnings, or post-event impact.

Scoring:
- 9‚Äì10: Major or severe floods or flash floods; large areas or populations affected; deaths, missing people, evacuations, red alerts, dam breaks, levee failures, or official high-level flood warnings.
- 7‚Äì8: Strong flood risk or heavy rainfall with credible probability of flooding; regional alerts; serious infrastructure damage or clear risk escalation.
- 5‚Äì6: Local floods with limited impact, or early signals where the flood angle is present but not yet severe.
- 1‚Äì4: Weather stories with weak or indirect flood relevance (e.g., storms but no flooding, vague references, minor local incidents).
- 0: NOT relevant to FloodLink (e.g., generic climate politics, non-weather news, economic climate, sports, entertainment).

Reply with ONLY a single integer (0‚Äì10).

Title: {title}
Summary: {summary}
"""

    try:
        response = client.chat.completions.create(
            model=XAI_MODEL,
            messages=[{"role": "user", "content": prompt}]
        )
        score_text = response.choices[0].message.content.strip()
        score = int(score_text)
        return score if 0 <= score <= 10 else 0
    except Exception as e:
        print(f"‚ùå Error scoring news: {e}")
        return 0

def summarize_news(title, summary, source):
    """
    Create a FloodLink tweet with clear FORECAST / POST-EVENT label.
    """
    client = openai.OpenAI(
        api_key=XAI_API_KEY,
        base_url="https://api.x.ai/v1"
    )

    prompt = f"""
You post as FloodLink, a global flood-risk early warning system on X.

Write ONE tweet about this article in EXACTLY this format:

<FLAG(optional)> <STATUS> <Location>: <short description>

Rules:
- STATUS = FORECAST (future or imminent risk, warnings, alerts)
          OR POST-EVENT (flood already happened: damage, deaths, rescues).
- If you can infer a clear country, put its flag emoji first (e.g. üá∫üá∏). Otherwise omit the flag.
- Location: short city/region/country name.
- Description: mention type (flood / flash flood / storm surge / landslide from rain)
  and key risk/impact. If you mention a source, keep it short at the end.
- Max 260 characters total.
- NO hashtags, NO emojis except the optional country flag.
- NO quotation marks.

Title: {title}
Summary: {summary}
Source: {source}
"""

    response = client.chat.completions.create(
        model=XAI_MODEL,
        messages=[{"role": "user", "content": prompt}]
    )

    tweet = response.choices[0].message.content.strip()
    tweet = tweet.replace('"', "").replace("'", "")
    return tweet[:280]


# =========================================================
#                       REPLIES
# =========================================================

def load_reply_log():
    if os.path.exists(REPLY_LOG_FILE):
        with open(REPLY_LOG_FILE, "r") as f:
            return json.load(f)
    return {}

def save_reply_log(log_data):
    with open(REPLY_LOG_FILE, "w") as f:
        json.dump(log_data, f, indent=4)

def count_replies_today(reply_log):
    today = datetime.utcnow().strftime("%Y-%m-%d")
    return sum(1 for entry in reply_log.values() if entry["date"] == today)

def fetch_latest_tweets(user_id, max_results=5):
    try:
        tweets = twitter_client.get_users_tweets(
            id=user_id,
            max_results=max_results,
            tweet_fields=["id", "text", "created_at"],
            exclude=["retweets", "replies"]
        )
        return tweets.data if tweets.data else []
    except tweepy.errors.TweepyException as e:
        print(f"‚ùå Error fetching tweets for {user_id}: {e}")
        return []

def pick_most_recent_tweet(all_tweets, reply_log):
    new_tweets = [t for t in all_tweets if str(t.id) not in reply_log]
    if not new_tweets:
        print("üîç No new tweets available to reply to.")
        return None
    return new_tweets[0]

def generate_grok_reply(tweet_text, username):
    """
    Reply as FloodLink with a short data / insight nugget about floods or extreme rainfall.
    """
    client = openai.OpenAI(
        api_key=XAI_API_KEY,
        base_url="https://api.x.ai/v1"
    )

    prompt = f"""
You are replying as FloodLink, a flood-risk early warning system on X.

Read this tweet from @{username} (likely about climate, disasters, weather or resilience)
and reply with ONE concise, data-driven insight related to:

- floods, flash floods, storm surge, rainfall extremes, river levels,
- early warning systems, flood forecasting, or urban flood resilience.

Rules:
- Under 240 characters.
- NO hashtags.
- NO emojis except country flags before location names, if used.
- Tone: factual, calm, slightly analytical. No hype.

Tweet:
\"\"\"{tweet_text}\"\"\"

Your reply (text only, no username prefix):
"""

    response = client.chat.completions.create(
        model=XAI_MODEL,
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content.strip()

def reply_to_random_tweet():
    reply_log = load_reply_log()
    if count_replies_today(reply_log) >= REPLY_TWEETS_LIMIT:
        print(f"üö´ Reached daily reply limit ({REPLY_TWEETS_LIMIT}).")
        return

    if not TARGET_ACCOUNTS:
        print("‚ö†Ô∏è No TARGET_ACCOUNTS configured for FloodLink replies.")
        return

    username = random.choice(list(TARGET_ACCOUNTS.keys()))
    user_id = TARGET_ACCOUNTS[username]
    print(f"üîç Fetching tweets from @{username}...")

    all_tweets = fetch_latest_tweets(user_id, max_results=5)
    if not all_tweets:
        return

    selected = pick_most_recent_tweet(all_tweets, reply_log)
    if not selected:
        return

    tweet_id = selected.id
    tweet_text = selected.text

    reply_text = generate_grok_reply(tweet_text, username)
    if not reply_text:
        print("‚ùå Failed to generate reply.")
        return

    try:
        twitter_client.create_tweet(
            text=f"@{username} {reply_text}",
            in_reply_to_tweet_id=tweet_id
        )
        print(f"‚úÖ Replied to @{username}: {reply_text}")

        reply_log[str(tweet_id)] = {
            "date": datetime.utcnow().strftime("%Y-%m-%d"),
            "username": username,
            "tweet_id": tweet_id
        }
        save_reply_log(reply_log)
    except tweepy.errors.TweepyException as e:
        print(f"‚ùå Error posting reply: {e}")

# =========================================================
#                      POSTING
# =========================================================

def post_tweet(tweet):
    print(f"üöÄ Attempting to tweet: {tweet}")
    try:
        resp = twitter_client.create_tweet(text=tweet)
        print(f"‚úÖ Tweet posted: {resp.data}")
        # small cooldown so runs don't spam
        time.sleep(60)
        return True
    except tweepy.errors.Forbidden as e:
        if "Status is a duplicate" in str(e):
            print("‚ö†Ô∏è Duplicate tweet detected. Skipping.")
        else:
            print(f"‚ùå Twitter API error: {e}")
        return False
    except tweepy.errors.TweepyException as e:
        print(f"‚ùå Other Tweepy error: {e}")
        return False

# =========================================================
#                        MAIN
# =========================================================

if __name__ == "__main__":
    print("üîç Loading previously processed FloodLink articles...")
    processed_articles = load_processed_articles()
    filtered_links = {a.get("link") for a in processed_articles if a.get("link")} if processed_articles else set()
    print(f"üìÇ {len(processed_articles)} flood-related articles already processed.")

    today = datetime.utcnow().strftime("%Y-%m-%d")
    today_news_count = count_news_tweets_today(processed_articles)
    reply_log = load_reply_log()
    today_reply_count = count_replies_today(reply_log)

    tweet_type = select_tweet_type()
    print(f"üîÄ Selected tweet type: {tweet_type}")

    # enforce limits
    if tweet_type == "news" and today_news_count >= NEWS_TWEETS_LIMIT:
        print(f"üö´ Reached daily news limit ({NEWS_TWEETS_LIMIT}).")
        exit(0)
    if tweet_type == "reply" and today_reply_count >= REPLY_TWEETS_LIMIT:
        print(f"üö´ Reached daily reply limit ({REPLY_TWEETS_LIMIT}).")
        exit(0)

    if tweet_type == "reply":
        reply_to_random_tweet()
        exit(0)

    if tweet_type == "news":
        latest_news = get_latest_news()
        print(f"üì∞ Found {len(latest_news)} recent articles.")

        scored_news = []
        seen_links = set()

        for title, link, source, summary in latest_news:
            if today_news_count >= NEWS_TWEETS_LIMIT:
                print(f"üö´ Stopping news: {today_news_count} tweets reached.")
                break

            if link in seen_links or link in filtered_links:
                print(f"‚è© Skipping duplicate article: {title}")
                continue
            seen_links.add(link)

            # similarity filter
            if is_similar_news(title, summary, processed_articles, threshold=0.5, limit=30):
                processed_articles.append({
                    "link": link,
                    "date": today,
                    "title": title,
                    "summary": summary,
                    "similarity_excluded": "Yes",
                    "score": 0,
                    "status": "skipped",
                    "tweet": None
                })
                continue

            score = get_news_relevance_score(title, summary)

            base_entry = {
                "link": link,
                "date": today,
                "title": title,
                "summary": summary,
                "similarity_excluded": "No",
                "score": score,
                "status": "processed",
                "tweet": None
            }
            processed_articles.append(base_entry)
            scored_news.append((score, title, link, source, summary))

        # sort by score, pick top few
        scored_news.sort(reverse=True, key=lambda x: x[0])
        top_articles = scored_news[:3]

        for score, title, link, source, summary in top_articles:
            if score >= TWEET_THRESHOLD:
                tweet = summarize_news(title, summary, source)
                if post_tweet(tweet):
                    today_news_count += 1
                    processed_articles.append({
                        "link": link,
                        "date": today,
                        "title": title,
                        "summary": summary,
                        "similarity_excluded": "No",
                        "score": score,
                        "status": "posted",
                        "tweet": tweet,
                        "type": "news"
                    })
            else:
                print(f"üö´ Article below threshold (score={score}): {title}")

    else:
        print("ü§ñ No tweet posted in this run (simulating human-like inactivity).")

    processed_articles = cleanup_old_articles(processed_articles)
    save_processed_articles(processed_articles)
    print("‚úÖ floodlink_news.json updated.")
